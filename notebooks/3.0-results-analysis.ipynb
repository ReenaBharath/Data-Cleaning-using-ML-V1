{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Results Analysis\n",
    "\n",
    "This notebook analyzes the results of our ML-based data cleaning pipeline, comparing the original and cleaned datasets to evaluate the effectiveness of our cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "from scipy import stats\n",
    "import yaml\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)  # Format floats to 3 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original and cleaned datasets\n",
    "try:\n",
    "    original_df = pd.read_csv('../data/raw/input_dataset.csv')\n",
    "    cleaned_df = pd.read_csv('../data/processed/cleaned_dataset.csv')\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(\"Could not find one of the required CSV files. Please ensure both input_dataset.csv and cleaned_dataset.csv exist in their respective directories.\") from e\n",
    "\n",
    "# Load configuration\n",
    "try:\n",
    "    with open('../configs/cleaning_config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"Could not find cleaning_config.yaml. Please ensure it exists in the configs directory.\")\n",
    "except yaml.YAMLError as e:\n",
    "    raise yaml.YAMLError(\"Error parsing cleaning_config.yaml. Please ensure it is valid YAML format.\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Statistics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_basic_stats(original_df, cleaned_df):\n",
    "    # Handle potential null values in text and hashtags columns\n",
    "    orig_text_len = original_df['text'].fillna('').str.len()\n",
    "    clean_text_len = cleaned_df['text'].fillna('').str.len()\n",
    "    orig_hashtags = original_df['hashtags'].fillna('').str.count('#')\n",
    "    clean_hashtags = cleaned_df['hashtags'].fillna('').str.count('#')\n",
    "    \n",
    "    stats_dict = {\n",
    "        'Metric': [\n",
    "            'Total Records',\n",
    "            'Records Removed',\n",
    "            'Removal Rate (%)',\n",
    "            'Average Text Length',\n",
    "            'Median Text Length', \n",
    "            'Average Hashtags per Record',\n",
    "            'Unique Country Codes',\n",
    "            'Missing Values (Total)'\n",
    "        ],\n",
    "        'Original': [\n",
    "            len(original_df),\n",
    "            0,\n",
    "            0,\n",
    "            orig_text_len.mean(),\n",
    "            orig_text_len.median(),\n",
    "            orig_hashtags.mean(),\n",
    "            original_df['country_code'].nunique(),\n",
    "            original_df.isnull().sum().sum()\n",
    "        ],\n",
    "        'Cleaned': [\n",
    "            len(cleaned_df),\n",
    "            len(original_df) - len(cleaned_df),\n",
    "            ((len(original_df) - len(cleaned_df)) / len(original_df) * 100),\n",
    "            clean_text_len.mean(),\n",
    "            clean_text_len.median(), \n",
    "            clean_hashtags.mean(),\n",
    "            cleaned_df['country_code'].nunique(),\n",
    "            cleaned_df.isnull().sum().sum()\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame and ensure numeric columns are rounded\n",
    "    stats_df = pd.DataFrame(stats_dict)\n",
    "    numeric_columns = ['Original', 'Cleaned']\n",
    "    stats_df[numeric_columns] = stats_df[numeric_columns].round(2)\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "basic_stats = calculate_basic_stats(original_df, cleaned_df)\n",
    "basic_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_quality(df, column='text'):\n",
    "    \"\"\"Analyze text quality metrics\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing text data\n",
    "        column (str): Name of the text column to analyze\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Series containing text quality metrics\n",
    "    \"\"\"\n",
    "    # Handle null values safely\n",
    "    text_series = df[column].fillna('')\n",
    "    \n",
    "    # Calculate metrics with error handling\n",
    "    try:\n",
    "        total_words = text_series.str.split().str.len().sum()\n",
    "        unique_words = len(set(' '.join(text_series[text_series != '']).split()))\n",
    "        avg_words = text_series.str.split().str.len().mean()\n",
    "        length_std = text_series.str.len().std()\n",
    "        empty_texts = df[column].isna().sum()\n",
    "        short_texts = (text_series.str.len() < 10).sum()\n",
    "        \n",
    "        metrics = {\n",
    "            'Total Words': total_words,\n",
    "            'Unique Words': unique_words,\n",
    "            'Average Words per Text': round(avg_words, 2),\n",
    "            'Text Length Std Dev': round(length_std, 2),\n",
    "            'Empty Texts': empty_texts,\n",
    "            'Short Texts (<10 chars)': short_texts\n",
    "        }\n",
    "        \n",
    "        return pd.Series(metrics)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {str(e)}\")\n",
    "        return pd.Series()\n",
    "\n",
    "# Compare text quality\n",
    "original_quality = analyze_text_quality(original_df)\n",
    "cleaned_quality = analyze_text_quality(cleaned_df)\n",
    "\n",
    "# Calculate percentage changes, handling division by zero\n",
    "quality_comparison = pd.DataFrame({\n",
    "    'Original': original_quality,\n",
    "    'Cleaned': cleaned_quality\n",
    "})\n",
    "\n",
    "# Calculate percentage change safely\n",
    "quality_comparison['Change (%)'] = quality_comparison.apply(\n",
    "    lambda row: round(((row['Cleaned'] - row['Original']) / row['Original'] * 100), 2) \n",
    "    if row['Original'] != 0 else float('inf'),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "quality_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization of Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text_length_distribution(original_df, cleaned_df):\n",
    "    # Handle potential missing or invalid data\n",
    "    if 'text' not in original_df.columns or 'text' not in cleaned_df.columns:\n",
    "        print(\"Error: 'text' column not found in dataframes\")\n",
    "        return\n",
    "        \n",
    "    # Create figure with larger size for better readability\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Calculate text lengths, handling NaN values\n",
    "    orig_lengths = original_df['text'].fillna('').str.len()\n",
    "    clean_lengths = cleaned_df['text'].fillna('').str.len()\n",
    "    \n",
    "    # Plot distributions with reasonable bin size\n",
    "    bins = min(50, int(max(orig_lengths.max(), clean_lengths.max()) / 20))\n",
    "    \n",
    "    # Plot original distribution\n",
    "    sns.histplot(data=orig_lengths, \n",
    "                label='Original', \n",
    "                alpha=0.5,\n",
    "                bins=bins,\n",
    "                color='blue')\n",
    "    \n",
    "    # Plot cleaned distribution\n",
    "    sns.histplot(data=clean_lengths,\n",
    "                label='Cleaned',\n",
    "                alpha=0.5, \n",
    "                bins=bins,\n",
    "                color='green')\n",
    "    \n",
    "    plt.title('Text Length Distribution Before and After Cleaning', pad=20)\n",
    "    plt.xlabel('Text Length (characters)', labelpad=10)\n",
    "    plt.ylabel('Count', labelpad=10)\n",
    "    plt.legend(title='Dataset')\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate the plot\n",
    "plot_text_length_distribution(original_df, cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_country_distribution(original_df, cleaned_df):\n",
    "    # Handle potential missing data\n",
    "    if 'country_code' not in original_df.columns or 'country_code' not in cleaned_df.columns:\n",
    "        print(\"Error: 'country_code' column not found in dataframes\")\n",
    "        return\n",
    "        \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Calculate value counts, handling NaN values\n",
    "    orig_counts = original_df['country_code'].fillna('Unknown').value_counts()\n",
    "    clean_counts = cleaned_df['country_code'].fillna('Unknown').value_counts()\n",
    "    \n",
    "    # Add original distribution\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=orig_counts.index,\n",
    "        y=orig_counts.values,\n",
    "        name='Original',\n",
    "        marker_color='lightblue'\n",
    "    ))\n",
    "    \n",
    "    # Add cleaned distribution\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=clean_counts.index,\n",
    "        y=clean_counts.values,\n",
    "        name='Cleaned',\n",
    "        marker_color='lightgreen'\n",
    "    ))\n",
    "    \n",
    "    # Update layout with more details\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': 'Country Code Distribution Before and After Cleaning',\n",
    "            'y':0.95,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "        xaxis_title='Country Code',\n",
    "        yaxis_title='Count',\n",
    "        barmode='group',\n",
    "        bargap=0.2,\n",
    "        bargroupgap=0.1,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"right\",\n",
    "            x=0.99\n",
    "        ),\n",
    "        plot_bgcolor='white'\n",
    "    )\n",
    "    \n",
    "    # Add gridlines for better readability\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Generate the plot\n",
    "plot_country_distribution(original_df, cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hashtag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hashtags(df, column='hashtags'):\n",
    "    # Handle empty dataframe case\n",
    "    if df.empty:\n",
    "        return pd.Series({\n",
    "            'Total Hashtags': 0,\n",
    "            'Unique Hashtags': 0, \n",
    "            'Average Hashtags per Record': 0,\n",
    "            'Records with Hashtags (%)': 0\n",
    "        })\n",
    "    \n",
    "    # Split hashtags and create a list of all hashtags\n",
    "    # Handle potential NaN values and ensure proper string splitting\n",
    "    all_hashtags = []\n",
    "    for tags in df[column].dropna():\n",
    "        if isinstance(tags, str):\n",
    "            # Split on whitespace and filter out empty strings\n",
    "            hashtags = [tag.strip() for tag in tags.split() if tag.strip()]\n",
    "            all_hashtags.extend(hashtags)\n",
    "    \n",
    "    # Count unique hashtags\n",
    "    unique_hashtags = len(set(all_hashtags))\n",
    "    \n",
    "    # Get top hashtags\n",
    "    top_hashtags = pd.Series(all_hashtags).value_counts().head(10)\n",
    "    \n",
    "    # Create word cloud only if there are hashtags\n",
    "    if all_hashtags:\n",
    "        wordcloud = WordCloud(width=800, height=400, \n",
    "                            background_color='white',\n",
    "                            min_font_size=10,\n",
    "                            max_font_size=50)\n",
    "        wordcloud.generate(' '.join(all_hashtags))\n",
    "        \n",
    "        # Plotting\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Plot top hashtags\n",
    "        top_hashtags.plot(kind='barh', ax=ax1)\n",
    "        ax1.set_title('Top 10 Hashtags')\n",
    "        ax1.set_xlabel('Frequency')\n",
    "        ax1.set_ylabel('Hashtags')\n",
    "        \n",
    "        # Plot word cloud\n",
    "        ax2.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title('Hashtag Word Cloud')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No hashtags found in the dataset\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_records = len(df)\n",
    "    records_with_hashtags = df[column].notna().sum()\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Total Hashtags': len(all_hashtags),\n",
    "        'Unique Hashtags': unique_hashtags,\n",
    "        'Average Hashtags per Record': len(all_hashtags) / total_records if total_records > 0 else 0,\n",
    "        'Records with Hashtags (%)': (records_with_hashtags / total_records * 100).round(2) if total_records > 0 else 0\n",
    "    })\n",
    "\n",
    "print(\"Original Dataset Hashtag Analysis:\")\n",
    "original_hashtag_stats = analyze_hashtags(original_df)\n",
    "print(original_hashtag_stats)\n",
    "\n",
    "print(\"\\nCleaned Dataset Hashtag Analysis:\")\n",
    "cleaned_hashtag_stats = analyze_hashtags(cleaned_df)\n",
    "print(cleaned_hashtag_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Development Status Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_development_status(original_df, cleaned_df):\n",
    "    # Check if development_status column exists in both dataframes\n",
    "    if 'development_status' not in original_df.columns or 'development_status' not in cleaned_df.columns:\n",
    "        raise ValueError(\"development_status column not found in one or both dataframes\")\n",
    "        \n",
    "    # Create comparison DataFrame\n",
    "    comparison = pd.DataFrame({\n",
    "        'Original': original_df['development_status'].value_counts(normalize=True) * 100,\n",
    "        'Cleaned': cleaned_df['development_status'].value_counts(normalize=True) * 100\n",
    "    }).round(2)\n",
    "    \n",
    "    # Calculate changes\n",
    "    comparison['Change (%)'] = (comparison['Cleaned'] - comparison['Original']).round(2)\n",
    "    \n",
    "    # Plotting\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Pie charts with better formatting\n",
    "    original_df['development_status'].value_counts().plot(\n",
    "        kind='pie', \n",
    "        autopct='%1.1f%%', \n",
    "        ax=ax1, \n",
    "        title='Original Distribution',\n",
    "        labeldistance=1.1)\n",
    "    ax1.set_ylabel('')  # Remove unnecessary y-label\n",
    "    \n",
    "    cleaned_df['development_status'].value_counts().plot(\n",
    "        kind='pie', \n",
    "        autopct='%1.1f%%', \n",
    "        ax=ax2, \n",
    "        title='Cleaned Distribution',\n",
    "        labeldistance=1.1)\n",
    "    ax2.set_ylabel('')  # Remove unnecessary y-label\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Add total counts\n",
    "    comparison['Original Count'] = original_df['development_status'].value_counts()\n",
    "    comparison['Cleaned Count'] = cleaned_df['development_status'].value_counts()\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Analyze development status changes\n",
    "development_status_comparison = analyze_development_status(original_df, cleaned_df)\n",
    "display(development_status_comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
