{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training for Data Cleaning Pipeline\n",
    "\n",
    "This notebook covers the training and fine-tuning of models used in our data cleaning pipeline. We'll focus on:\n",
    "1. Loading and preparing the data\n",
    "2. Setting up the models\n",
    "3. Training the development status classifier\n",
    "4. Model evaluation and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "import logging\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load configuration\n",
    "def load_config():\n",
    "    with open('../configs/model_config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "config = load_config()\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"Load and prepare the dataset for model training\"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    df['country_code'] = df['country_code'].fillna('UNKNOWN')\n",
    "    \n",
    "    # Create binary labels for development status\n",
    "    df['label'] = df['development_status'].map({\n",
    "        'Developed': 1,\n",
    "        'Developing': 0\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df = load_and_prepare_data('../data/raw/input_dataset.csv')\n",
    "print(f\"Loaded {len(df)} records\")\n",
    "print(\"\\nSample data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config['model_params']['device'])\n",
    "        self.setup_models()\n",
    "        \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize models and tokenizers\"\"\"\n",
    "        # Development status model\n",
    "        model_config = self.config['model_params']['development_status']\n",
    "        self.dev_status_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_config['model_name'],\n",
    "            cache_dir=model_config['cache_dir']\n",
    "        )\n",
    "        self.dev_status_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_config['model_name'],\n",
    "            num_labels=model_config['num_labels'],\n",
    "            cache_dir=model_config['cache_dir']\n",
    "        ).to(self.device)\n",
    "        \n",
    "    def prepare_dataset(self, df, text_col, label_col):\n",
    "        \"\"\"Prepare dataset for training\"\"\"\n",
    "        # Create features\n",
    "        features = [{\n",
    "            'text': str(row[text_col]),\n",
    "            'label': int(row[label_col])\n",
    "        } for _, row in df.iterrows()]\n",
    "        \n",
    "        # Convert to Dataset format\n",
    "        dataset = Dataset.from_list(features)\n",
    "        \n",
    "        # Tokenize function\n",
    "        def tokenize_function(examples):\n",
    "            return self.dev_status_tokenizer(\n",
    "                examples['text'],\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.config['model_params']['max_length']\n",
    "            )\n",
    "        \n",
    "        # Tokenize dataset\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        \n",
    "        return tokenized_dataset\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ModelTrainer(config)\n",
    "print(\"Model trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Splitting and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data\n",
    "train_df, eval_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=config['model_params']['seed']\n",
    ")\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = trainer.prepare_dataset(train_df, 'text', 'label')\n",
    "eval_dataset = trainer.prepare_dataset(eval_df, 'text', 'label')\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def setup_training(config, train_dataset, eval_dataset):\n",
    "    \"\"\"Setup training arguments and trainer\"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"../models/dev_status_model\",\n",
    "        num_train_epochs=config['model_params']['num_epochs'],\n",
    "        per_device_train_batch_size=config['model_params']['batch_size'],\n",
    "        per_device_eval_batch_size=config['model_params']['batch_size'],\n",
    "        learning_rate=config['model_params']['learning_rate'],\n",
    "        warmup_steps=config['model_params']['warmup_steps'],\n",
    "        weight_decay=config['model_params']['weight_decay'],\n",
    "        logging_dir=\"../logs\",\n",
    "        logging_steps=config['logging']['logging_steps'],\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "    \n",
    "    # Setup trainer\n",
    "    trainer = Trainer(\n",
    "        model=trainer.dev_status_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=trainer.dev_status_tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(trainer.dev_status_tokenizer)\n",
    "    )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Setup training\n",
    "model_trainer = setup_training(config, train_dataset, eval_dataset)\n",
    "print(\"Training setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Analysis and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_data_distribution(df):\n",
    "    \"\"\"Analyze the distribution of labels and data characteristics\"\"\"\n",
    "    # Plot label distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x='development_status')\n",
    "    plt.title('Distribution of Development Status')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    # Text length distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    sns.histplot(data=df, x='text_length', bins=50)\n",
    "    plt.title('Distribution of Text Length')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    print(df['development_status'].value_counts(normalize=True))\n",
    "    \n",
    "    print(\"\\nText Length Statistics:\")\n",
    "    print(df['text_length'].describe())\n",
    "\n",
    "# Analyze data\n",
    "analyze_data_distribution(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 }
}
