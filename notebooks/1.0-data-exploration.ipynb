{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Data Exploration and Initial Analysis\n",
    "\n",
    "This notebook performs initial exploration of our dataset and analyzes the quality of different columns before implementing the cleaning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "import warnings\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure warnings and logging\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set plotting style and figure size defaults\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# Try loading spaCy model with error handling\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    logger.error(\"spaCy model 'en_core_web_sm' not found. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_sm'])\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading spaCy model: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    data_path = Path('../data/raw/zero_waste.csv')\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found at {data_path}\")\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Validate that required columns exist based on config\n",
    "    required_columns = ['text', 'hashtags', 'place_country_code', 'Developed / Developing']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"Dataset Shape:\", df.shape)\n",
    "    print(\"\\nColumns:\", df.columns.tolist())\n",
    "    print(\"\\nDataset Info:\")\n",
    "    df.info()\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Log successful data load\n",
    "    logger.info(f\"Successfully loaded dataset with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Error loading data: {str(e)}\")\n",
    "    raise\n",
    "except ValueError as e:\n",
    "    logger.error(f\"Data validation error: {str(e)}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Unexpected error loading data: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_column(texts):\n",
    "    \"\"\"Analyze text column for various metrics\"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(texts, pd.Series):\n",
    "        raise TypeError(\"Input must be a pandas Series\")\n",
    "    \n",
    "    # Handle empty input\n",
    "    if len(texts) == 0:\n",
    "        raise ValueError(\"Input Series is empty\")\n",
    "        \n",
    "    try:\n",
    "        metrics = {\n",
    "            'total_rows': len(texts),\n",
    "            'null_count': texts.isnull().sum(),\n",
    "            'empty_count': texts.str.strip().eq('').sum(),\n",
    "            'unique_count': texts.nunique(),\n",
    "            'avg_length': texts.str.len().mean(),\n",
    "            'min_length': texts.str.len().min(),\n",
    "            'max_length': texts.str.len().max(),\n",
    "        }\n",
    "        \n",
    "        # Validate metrics\n",
    "        if any(pd.isna(value) for value in metrics.values()):\n",
    "            raise ValueError(\"Error calculating metrics - check input data\")\n",
    "            \n",
    "        # Calculate and plot length distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        texts.str.len().hist(bins=50)\n",
    "        plt.title('Text Length Distribution')\n",
    "        plt.xlabel('Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "        \n",
    "        return pd.Series(metrics)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing text column: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Analyze text column\n",
    "    text_metrics = analyze_text_column(df['text'])\n",
    "    print(\"Text Column Metrics:\")\n",
    "    print(text_metrics)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to analyze text column: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text_issues(texts):\n",
    "    \"\"\"Detect various issues in text data\"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(texts, pd.Series):\n",
    "        raise TypeError(\"Input must be a pandas Series\")\n",
    "        \n",
    "    issues = {\n",
    "        'urls': [],\n",
    "        'non_english': [],\n",
    "        'short_text': [], \n",
    "        'special_chars': [],\n",
    "        'rt_prefix': []\n",
    "    }\n",
    "    \n",
    "    # Get validation parameters from config\n",
    "    min_words = 3  # From cleaning_config validation min_words\n",
    "    \n",
    "    for idx, text in enumerate(texts):\n",
    "        if pd.isna(text):\n",
    "            continue\n",
    "            \n",
    "        text = str(text).strip()\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        # Check for URLs using regex from cleaning config\n",
    "        if re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text):\n",
    "            issues['urls'].append(idx)\n",
    "        \n",
    "        # Check language using language detection model\n",
    "        try:\n",
    "            if detect(text) != 'en':\n",
    "                issues['non_english'].append(idx)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Language detection failed for text at index {idx}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        # Check minimum word length\n",
    "        if len(text.split()) < min_words:\n",
    "            issues['short_text'].append(idx)\n",
    "        \n",
    "        # Check special characters, excluding allowed punctuation\n",
    "        if re.search(r'[^\\w\\s.,!?\\'\"-]', text):\n",
    "            issues['special_chars'].append(idx)\n",
    "        \n",
    "        # Check RT prefix case-insensitively\n",
    "        if text.lower().startswith('rt '):\n",
    "            issues['rt_prefix'].append(idx)\n",
    "    \n",
    "    # Calculate issue counts\n",
    "    issue_counts = {k: len(v) for k, v in issues.items()}\n",
    "    \n",
    "    # Validate results\n",
    "    if sum(issue_counts.values()) == 0 and len(texts) > 0:\n",
    "        logger.warning(\"No issues detected - verify detection logic\")\n",
    "        \n",
    "    return issue_counts\n",
    "\n",
    "try:\n",
    "    # Analyze text issues\n",
    "    text_issues = detect_text_issues(df['text'])\n",
    "    print(\"\\nText Issues Found:\")\n",
    "    for issue, count in text_issues.items():\n",
    "        percentage = count/len(df)*100 if len(df) > 0 else 0\n",
    "        print(f\"{issue}: {count} instances ({percentage:.2f}%)\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to detect text issues: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hashtag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hashtags(hashtags):\n",
    "    \"\"\"Analyze hashtag patterns and issues\"\"\"\n",
    "    if not isinstance(hashtags, pd.Series):\n",
    "        raise TypeError(\"Input must be a pandas Series\")\n",
    "        \n",
    "    # Split hashtags into individual tags\n",
    "    all_tags = []\n",
    "    for tags in hashtags.dropna():\n",
    "        if isinstance(tags, str):\n",
    "            # Split on whitespace and filter out empty strings\n",
    "            tags_list = [tag.strip() for tag in tags.split() if tag.strip()]\n",
    "            # Validate each tag starts with #\n",
    "            tags_list = [tag if tag.startswith('#') else f'#{tag}' for tag in tags_list]\n",
    "            all_tags.extend(tags_list)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tag_counts = Counter(all_tags)\n",
    "    \n",
    "    try:\n",
    "        avg_per_row = len(all_tags) / len(hashtags) if len(hashtags) > 0 else 0\n",
    "    except ZeroDivisionError:\n",
    "        avg_per_row = 0\n",
    "        \n",
    "    metrics = {\n",
    "        'total_hashtags': len(all_tags),\n",
    "        'unique_hashtags': len(tag_counts),\n",
    "        'avg_per_row': avg_per_row,\n",
    "        'rows_with_tags': hashtags.notna().sum(),\n",
    "        'empty_rows': hashtags.isna().sum(),\n",
    "        'max_tags_per_row': max((len(str(tags).split()) for tags in hashtags.dropna()), default=0)\n",
    "    }\n",
    "    \n",
    "    # Plot top hashtags if there are any\n",
    "    if tag_counts:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        top_tags = pd.Series(dict(tag_counts.most_common(20)))\n",
    "        top_tags.plot(kind='bar')\n",
    "        plt.title('Top 20 Hashtags')\n",
    "        plt.xlabel('Hashtag')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No hashtags found to plot\")\n",
    "    \n",
    "    return pd.Series(metrics)\n",
    "\n",
    "try:\n",
    "    # Analyze hashtags\n",
    "    hashtag_metrics = analyze_hashtags(df['hashtags'])\n",
    "    print(\"\\nHashtag Metrics:\")\n",
    "    print(hashtag_metrics)\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing hashtags: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Country Code Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_country_codes(codes):\n",
    "    \"\"\"Analyze country code distribution and validity\"\"\"\n",
    "    # Get valid country codes from ISO 3166-1 alpha-2 standard\n",
    "    # Using a more comprehensive list of valid country codes\n",
    "    valid_codes = set([\n",
    "        \"AF\", \"AL\", \"DZ\", \"AS\", \"AD\", \"AO\", \"AI\", \"AQ\", \"AG\", \"AR\", \"AM\", \"AW\", \"AU\", \"AT\", \"AZ\",\n",
    "        \"BS\", \"BH\", \"BD\", \"BB\", \"BY\", \"BE\", \"BZ\", \"BJ\", \"BM\", \"BT\", \"BO\", \"BA\", \"BW\", \"BV\", \"BR\",\n",
    "        \"IO\", \"BN\", \"BG\", \"BF\", \"BI\", \"KH\", \"CM\", \"CA\", \"CV\", \"KY\", \"CF\", \"TD\", \"CL\", \"CN\", \"CX\",\n",
    "        \"CC\", \"CO\", \"KM\", \"CG\", \"CD\", \"CK\", \"CR\", \"CI\", \"HR\", \"CU\", \"CY\", \"CZ\", \"DK\", \"DJ\", \"DM\",\n",
    "        \"DO\", \"EC\", \"EG\", \"SV\", \"GQ\", \"ER\", \"EE\", \"ET\", \"FK\", \"FO\", \"FJ\", \"FI\", \"FR\", \"GF\", \"PF\",\n",
    "        \"TF\", \"GA\", \"GM\", \"GE\", \"DE\", \"GH\", \"GI\", \"GR\", \"GL\", \"GD\", \"GP\", \"GU\", \"GT\", \"GN\", \"GW\",\n",
    "        \"GY\", \"HT\", \"HM\", \"VA\", \"HN\", \"HK\", \"HU\", \"IS\", \"IN\", \"ID\", \"IR\", \"IQ\", \"IE\", \"IL\", \"IT\",\n",
    "        \"JM\", \"JP\", \"JO\", \"KZ\", \"KE\", \"KI\", \"KP\", \"KR\", \"KW\", \"KG\", \"LA\", \"LV\", \"LB\", \"LS\", \"LR\",\n",
    "        \"LY\", \"LI\", \"LT\", \"LU\", \"MO\", \"MK\", \"MG\", \"MW\", \"MY\", \"MV\", \"ML\", \"MT\", \"MH\", \"MQ\", \"MR\",\n",
    "        \"MU\", \"YT\", \"MX\", \"FM\", \"MD\", \"MC\", \"MN\", \"MS\", \"MA\", \"MZ\", \"MM\", \"NA\", \"NR\", \"NP\", \"NL\",\n",
    "        \"NC\", \"NZ\", \"NI\", \"NE\", \"NG\", \"NU\", \"NF\", \"MP\", \"NO\", \"OM\", \"PK\", \"PW\", \"PS\", \"PA\", \"PG\",\n",
    "        \"PY\", \"PE\", \"PH\", \"PN\", \"PL\", \"PT\", \"PR\", \"QA\", \"RE\", \"RO\", \"RU\", \"RW\", \"SH\", \"KN\", \"LC\",\n",
    "        \"PM\", \"VC\", \"WS\", \"SM\", \"ST\", \"SA\", \"SN\", \"SC\", \"SL\", \"SG\", \"SK\", \"SI\", \"SB\", \"SO\", \"ZA\",\n",
    "        \"GS\", \"ES\", \"LK\", \"SD\", \"SR\", \"SJ\", \"SZ\", \"SE\", \"CH\", \"SY\", \"TW\", \"TJ\", \"TZ\", \"TH\", \"TL\",\n",
    "        \"TG\", \"TK\", \"TO\", \"TT\", \"TN\", \"TR\", \"TM\", \"TC\", \"TV\", \"UG\", \"UA\", \"AE\", \"GB\", \"US\", \"UM\",\n",
    "        \"UY\", \"UZ\", \"VU\", \"VE\", \"VN\", \"VG\", \"VI\", \"WF\", \"EH\", \"YE\", \"ZM\", \"ZW\", \"UK\"  # Including UK as common variant\n",
    "    ])\n",
    "    \n",
    "    # Convert codes to uppercase for consistent comparison\n",
    "    codes = codes.str.upper()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'total_rows': len(codes),\n",
    "        'null_count': codes.isnull().sum(),\n",
    "        'unique_codes': codes.nunique(),\n",
    "        'valid_codes': sum(1 for code in codes.dropna() if code in valid_codes),\n",
    "        'invalid_codes': sum(1 for code in codes.dropna() if code not in valid_codes),\n",
    "        'most_common_code': codes.mode()[0] if not codes.empty else 'None'\n",
    "    }\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_non_null = len(codes.dropna())\n",
    "    if total_non_null > 0:\n",
    "        metrics['valid_codes_pct'] = (metrics['valid_codes'] / total_non_null) * 100\n",
    "        metrics['invalid_codes_pct'] = (metrics['invalid_codes'] / total_non_null) * 100\n",
    "    else:\n",
    "        metrics['valid_codes_pct'] = 0\n",
    "        metrics['invalid_codes_pct'] = 0\n",
    "        \n",
    "    # Plot code distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    value_counts = codes.value_counts().head(20)\n",
    "    ax = value_counts.plot(kind='bar')\n",
    "    plt.title('Country Code Distribution (Top 20)')\n",
    "    plt.xlabel('Country Code')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(value_counts):\n",
    "        ax.text(i, v, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pd.Series(metrics)\n",
    "\n",
    "try:\n",
    "    # Analyze country codes\n",
    "    country_metrics = analyze_country_codes(df['country_code'])\n",
    "    print(\"\\nCountry Code Metrics:\")\n",
    "    print(country_metrics)\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing country codes: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Development Status Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_development_status(status):\n",
    "    \"\"\"Analyze development status distribution and standardization needs\"\"\"\n",
    "    if not isinstance(status, pd.Series):\n",
    "        raise TypeError(\"Input must be a pandas Series\")\n",
    "        \n",
    "    # Standardize status for analysis\n",
    "    status = status.str.lower().fillna('unknown')\n",
    "    \n",
    "    # Get valid categories from config\n",
    "    valid_categories = ['developed', 'developing']\n",
    "    \n",
    "    metrics = {\n",
    "        'total_rows': len(status),\n",
    "        'null_count': (status == 'unknown').sum(),\n",
    "        'unique_values': status.nunique(),\n",
    "        'developed_count': sum(1 for s in status if any(term in s for term in ['developed', 'advanced'])),\n",
    "        'developing_count': sum(1 for s in status if any(term in s for term in ['developing', 'emerging'])),\n",
    "        'unclear_count': sum(1 for s in status if not any(term in s for term in ['developed', 'developing', 'advanced', 'emerging']))\n",
    "    }\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total = metrics['total_rows']\n",
    "    metrics['developed_pct'] = (metrics['developed_count'] / total) * 100\n",
    "    metrics['developing_pct'] = (metrics['developing_count'] / total) * 100\n",
    "    metrics['unclear_pct'] = (metrics['unclear_count'] / total) * 100\n",
    "    \n",
    "    # Plot status distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    status_counts = status.value_counts()\n",
    "    colors = ['#2ecc71', '#e74c3c', '#95a5a6']  # Green, Red, Gray\n",
    "    plt.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "    plt.title('Development Status Distribution')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pd.Series(metrics)\n",
    "\n",
    "try:\n",
    "    # Analyze development status\n",
    "    status_metrics = analyze_development_status(df['development_status'])\n",
    "    print(\"\\nDevelopment Status Metrics:\")\n",
    "    print(status_metrics)\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing development status: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationships between columns\n",
    "def analyze_cross_columns(df):\n",
    "    \"\"\"Analyze relationships between different columns\"\"\"\n",
    "    try:\n",
    "        # Validate input dataframe\n",
    "        required_columns = ['text', 'hashtags', 'country_code', 'development_status']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(\"Missing required columns in dataframe\")\n",
    "            \n",
    "        # Text length by country\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        text_lengths = df['text'].fillna('').str.len()\n",
    "        sns.boxplot(x='country_code', y=text_lengths, data=df)\n",
    "        plt.title('Text Length Distribution by Country')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "        \n",
    "        # Hashtag count by development status\n",
    "        df['hashtag_count'] = df['hashtags'].fillna('').str.split().str.len()\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x='development_status', y='hashtag_count', data=df)\n",
    "        plt.title('Hashtag Count by Development Status')\n",
    "        plt.show()\n",
    "        \n",
    "        # Correlation matrix\n",
    "        correlation_data = pd.DataFrame({\n",
    "            'text_length': text_lengths,\n",
    "            'hashtag_count': df['hashtag_count'],\n",
    "            'is_developed': (df['development_status'].str.lower().str.contains('developed', na=False)).astype(int)\n",
    "        })\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(correlation_data.corr(), annot=True, cmap='coolwarm')\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in cross-column analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Perform cross-column analysis\n",
    "    analyze_cross_columns(df)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to perform cross-column analysis: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
